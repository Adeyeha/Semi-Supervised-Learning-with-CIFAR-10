{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from tsne_torch import TorchTSNE\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "from gradcam.utils import visualize_cam\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing: normalization and data augmentation\n",
    "def get_transforms():\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    return transform_train, transform_test\n",
    "\n",
    "def display_image(image, title, name):\n",
    "    plt.imshow(image.cpu().permute(1, 2, 0))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.savefig(f\"{name}_image.jpg\")\n",
    "\n",
    "def cutmix(data, targets, alpha=1.0):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    image_h, image_w = data.shape[2:]\n",
    "    cx = np.random.uniform(0, image_w)\n",
    "    cy = np.random.uniform(0, image_h)\n",
    "    w = image_w * np.sqrt(1 - lam)\n",
    "    h = image_h * np.sqrt(1 - lam)\n",
    "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
    "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
    "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
    "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
    "\n",
    "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def cutmix_criterion(criterion, outputs, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    return lam * criterion(outputs, targets1) + (1 - lam) * criterion(outputs, targets2)\n",
    "\n",
    "\n",
    "def mixup(inputs, targets, alpha):\n",
    "    batch_size = inputs.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_inputs = inputs[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    mixed_inputs = lam * inputs + (1 - lam) * shuffled_inputs\n",
    "    return mixed_inputs, (targets, shuffled_targets, lam)\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, preds, targets):\n",
    "    targets1, targets2, lam = targets\n",
    "    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n",
    "\n",
    "# Plot training and validation curves\n",
    "def plot_curves(train_losses, val_losses,name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(f\"{name}_curves.jpg\")\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "def plot_tsne(features, labels,name):\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=1000)\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "    plt.figure()\n",
    "    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels, cmap=plt.cm.get_cmap(\"jet\", 10), marker='o', s=20)\n",
    "    plt.colorbar(ticks=range(10))\n",
    "    plt.clim(-0.5, 9.5)\n",
    "    plt.show()\n",
    "    plt.savefig(f\"{name}_plot_tsne.jpg\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred,name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    plt.savefig(f\"{name}_confusion.jpg\")\n",
    "\n",
    "def run_experiment(name,trainset, testset, data_proportion, num_epochs, use_supervised, use_mixup, use_cutmix, lambda_=0.1, alpha=1.0,unlabeled_proportion=0.1):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(weights=None, num_classes=10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    if use_supervised:\n",
    "        train_size = int(train_test_split * len(trainset) * data_proportion)\n",
    "        val_size = int((len(trainset) * data_proportion) - train_size)\n",
    "        \n",
    "#         print(f\"train_size = {train_size}\")\n",
    "#         print(f\"val_size = {val_size}\")\n",
    "\n",
    "        train_indices = list(range(0, train_size))\n",
    "        train_subset = torch.utils.data.Subset(trainset, train_indices)\n",
    "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "        val_indices = list(range(train_size, train_size + val_size))\n",
    "        val_subset = torch.utils.data.Subset(trainset, val_indices)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=100, shuffle=False, num_workers=2)\n",
    "        \n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Semi-supervised learning\n",
    "        labeled_size = int(train_test_split * len(trainset) * ( 1 - unlabeled_proportion) * data_proportion)\n",
    "        unlabeled_size = int(train_test_split * len(trainset) * unlabeled_proportion * data_proportion)\n",
    "        val_size = int((len(trainset) * data_proportion)  - labeled_size - unlabeled_size)\n",
    "        \n",
    "#         print(f\"labeled_size = {labeled_size}\")\n",
    "#         print(f\"unlabeled_size = {unlabeled_size}\")\n",
    "#         print(f\"val_size = {val_size}\")\n",
    "\n",
    "        \n",
    "        labeled_indices = list(range(0, labeled_size))\n",
    "        labeled_subset = torch.utils.data.Subset(trainset, labeled_indices)\n",
    "        labeled_loader = torch.utils.data.DataLoader(labeled_subset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "        unlabeled_indices = list(range(labeled_size, labeled_size + unlabeled_size))\n",
    "        unlabeled_subset = torch.utils.data.Subset(trainset, unlabeled_indices)\n",
    "        unlabeled_loader = torch.utils.data.DataLoader(unlabeled_subset, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "        val_indices = list(range(labeled_size + unlabeled_size, labeled_size + unlabeled_size + val_size))\n",
    "        val_subset = torch.utils.data.Subset(trainset, val_indices)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "    \n",
    "    display_mixup = True\n",
    "    display_cutmix = True\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        if use_supervised:\n",
    "            for i, (data, targets) in enumerate(train_loader):\n",
    "                \n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                if use_mixup:\n",
    "                    mixup_data, mixup_targets = mixup(data.clone(), targets.clone(), alpha)\n",
    "                    mixup_data = mixup_data.to(device)\n",
    "                    mixup_outputs = model(mixup_data)\n",
    "                    if display_mixup:\n",
    "                        display_image(mixup_data[0], \"MixUp Sample Image\", f\"{name}_mixup\")\n",
    "                        display_mixup = False\n",
    "                    mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_targets)\n",
    "\n",
    "                    loss += mixup_loss\n",
    "\n",
    "                if use_cutmix:\n",
    "                    cutmix_data, cutmix_targets = cutmix(data.clone(), targets.clone(), alpha)\n",
    "                    cutmix_data = cutmix_data.to(device)\n",
    "                    cutmix_outputs = model(cutmix_data)\n",
    "                    if display_cutmix:\n",
    "                        display_image(cutmix_data[0], \"CutMix Sample Image\", f\"{name}_cutmix\")\n",
    "                        display_cutmix = False\n",
    "                    cutmix_loss = cutmix_criterion(criterion, cutmix_outputs, cutmix_targets)\n",
    "\n",
    "                    loss += cutmix_loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        else:\n",
    "            # Semi-supervised learning\n",
    "            for i, ((labeled_data, labeled_targets), (unlabeled_data, _)) in enumerate(zip(labeled_loader, unlabeled_loader)):\n",
    "                labeled_data, labeled_targets = labeled_data.to(device), labeled_targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                labeled_outputs = model(labeled_data)\n",
    "                labeled_loss = criterion(labeled_outputs, labeled_targets)\n",
    "\n",
    "                unlabeled_data = unlabeled_data.to(device)\n",
    "                unlabeled_outputs = model(unlabeled_data)\n",
    "                pseudo_labels = torch.argmax(unlabeled_outputs, dim=1)\n",
    "                unlabeled_loss = criterion(unlabeled_outputs, pseudo_labels)\n",
    "\n",
    "                loss = labeled_loss + lambda_ * unlabeled_loss\n",
    "\n",
    "                if use_mixup:\n",
    "                    mixup_data, mixup_targets = mixup(labeled_data.clone(), labeled_targets.clone(), alpha)\n",
    "                    mixup_data = mixup_data.to(device)\n",
    "                    mixup_outputs = model(mixup_data)\n",
    "                    if display_mixup:\n",
    "                        display_image(mixup_data[0], \"MixUp Sample Image\", f\"{name}_mixup\")\n",
    "                        display_mixup = False\n",
    "                    mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_targets)\n",
    "\n",
    "                    loss += mixup_loss\n",
    "\n",
    "                if use_cutmix:\n",
    "                    cutmix_data, cutmix_targets = cutmix(labeled_data.clone(), labeled_targets.clone(), alpha)\n",
    "                    cutmix_data = cutmix_data.to(device)\n",
    "                    cutmix_outputs = model(cutmix_data)\n",
    "                    if display_cutmix:\n",
    "                        display_image(cutmix_data[0], \"CutMix Sample Image\", f\"{name}_cutmix\")\n",
    "                        display_cutmix = False\n",
    "                    cutmix_loss = cutmix_criterion(criterion, cutmix_outputs, cutmix_targets)\n",
    "\n",
    "                    loss += cutmix_loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            train_losses.append(running_loss / len(labeled_loader))\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "    # Plot training and validation curves\n",
    "    plot_curves(train_losses, val_losses,name)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    print(f'Test accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    plot_confusion_matrix(y_true, y_pred,name)\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "# Set your desired unlabeled_ratio and data_proportion values\n",
    "unlabeled_ratio = 0.1\n",
    "data_proportion = 0.2\n",
    "train_test_split = 0.8\n",
    "num_epochs = 50\n",
    "lambda_ = 0.3\n",
    "alpha = 0.3\n",
    "ur=0.1\n",
    "\n",
    "# Load data and run the experiment\n",
    "transform_train, transform_test = get_transforms()\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "unsup = run_experiment(\"Baseline\",trainset, testset, data_proportion, num_epochs, use_supervised=True, use_mixup=False, use_cutmix=False, lambda_ = lambda_, alpha = alpha, unlabeled_proportion = ur)\n",
    "# unsup = run_experiment(trainset, testset, data_proportion, num_epochs, use_supervised=True, use_mixup=False, use_cutmix=True, lambda_ = lambda_, alpha = alpha, unlabeled_ratio = ur)\n",
    "# unsup = run_experiment(trainset, testset, data_proportion, num_epochs, use_supervised=True, use_mixup=True, use_cutmix=False, lambda_ = lambda_, alpha = alpha, unlabeled_ratio = ur)\n",
    "# unsup = run_experiment(trainset, testset, data_proportion, num_epochs, use_supervised=True, use_mixup=True, use_cutmix=True, lambda_ = lambda_, alpha = alpha, unlabeled_ratio = ur)\n",
    "res = {}\n",
    "for x in range(1,9,2):\n",
    "    print(f\"--------------{x*unlabeled_ratio *100}\")\n",
    "    ur = round((unlabeled_ratio * x),1)\n",
    "    sub_result = []\n",
    "    sub_result.append(run_experiment(f\"EM_{ur}\",trainset, testset, data_proportion, num_epochs, use_supervised=False, use_mixup=False, use_cutmix=False, lambda_ = lambda_, alpha = alpha, unlabeled_proportion = ur))\n",
    "    sub_result.append(run_experiment(f\"EM_CutMix_{ur}\",trainset, testset, data_proportion, num_epochs, use_supervised=False, use_mixup=False, use_cutmix=True, lambda_ = lambda_, alpha = alpha, unlabeled_proportion = ur))\n",
    "    sub_result.append(run_experiment(f\"EM_MixUp_{ur}\",trainset, testset, data_proportion, num_epochs, use_supervised=False, use_mixup=True, use_cutmix=False, lambda_ = lambda_, alpha = alpha, unlabeled_proportion = ur))\n",
    "    sub_result.append(run_experiment(f\"EM_CutMix_MixUp{ur}\",trainset, testset, data_proportion, num_epochs, use_supervised=False, use_mixup=True, use_cutmix=True, lambda_ = lambda_, alpha = alpha, unlabeled_proportion = ur))\n",
    "    res[ur] = sub_result"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
